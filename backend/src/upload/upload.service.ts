import { Injectable, Inject } from '@nestjs/common';
import { InjectRepository } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import * as fs from 'fs/promises';
import { PdfDocument } from './pdf-document.entity';
import { OllamaEmbeddings, ChatOllama } from '@langchain/ollama';
import { PromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { RecursiveCharacterTextSplitter } from '@langchain/textsplitters';
// ğŸ‘‡ [ë³€ê²½] CACHE_MANAGER ì„í¬íŠ¸ í•„ìˆ˜
import { CACHE_MANAGER } from '@nestjs/cache-manager';
import type { Cache } from 'cache-manager';

interface PDFPageItem {
  str: string;
}

interface SearchResult {
  id: number;
  filename: string;
  originalName: string;
  content: string;
  similarity: number;
}

// ğŸ‘‡ [ì¶”ê°€] Redisì— ì €ì¥í•  ëŒ€í™” ë©”ì‹œì§€ íƒ€ì… ì •ì˜
interface ChatMessage {
  role: 'user' | 'assistant';
  content: string;
}

@Injectable()
export class UploadService {
  private embeddings: OllamaEmbeddings;
  private chatModel: ChatOllama;

  // âŒ [ì‚­ì œ] ê¸°ì¡´ ì¸ë©”ëª¨ë¦¬ ì €ì¥ì†Œ(Map)ëŠ” ì´ì œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  // private sessions: Map<string, BaseMessage[]> = new Map();

  constructor(
    @InjectRepository(PdfDocument)
    private pdfRepository: Repository<PdfDocument>,
    // ğŸ‘‡ [ì¶”ê°€] Redis Cache Manager ì£¼ì…
    @Inject(CACHE_MANAGER) private cacheManager: Cache,
  ) {
    // 1. ì„ë² ë”© ëª¨ë¸
    this.embeddings = new OllamaEmbeddings({
      model: 'nomic-embed-text',
      baseUrl: 'http://localhost:11434',
      numCtx: 2048,
    } as any);

    // 2. ì±„íŒ… ëª¨ë¸ (Llama 3)
    this.chatModel = new ChatOllama({
      model: 'llama3',
      baseUrl: 'http://localhost:11434',
      temperature: 0.3,
      numCtx: 2048,
      num_predict: 512,
    } as any);
  }

  // ... (parsePdf, getEmbedding, saveFile, search ë©”ì„œë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµí•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤) ...

  async parsePdf(filePath: string): Promise<string> {
    const pdfjsLib = await import('pdfjs-dist/legacy/build/pdf.mjs');
    const dataBuffer = await fs.readFile(filePath);
    const uint8Array = new Uint8Array(dataBuffer);

    const loadingTask = pdfjsLib.getDocument({
      data: uint8Array,
      useSystemFonts: true,
      disableFontFace: true,
      standardFontDataUrl: 'node_modules/pdfjs-dist/standard_fonts/',
    });

    const doc = await loadingTask.promise;
    const maxPages = doc.numPages;
    const textContents: string[] = [];

    for (let i = 1; i <= maxPages; i++) {
      const page = await doc.getPage(i);
      const content = await page.getTextContent();
      const text = content.items
        .map((item: unknown) => (item as PDFPageItem).str)
        .join(' ');
      textContents.push(text);
    }
    return textContents.join('\n');
  }

  async getEmbedding(text: string): Promise<number[]> {
    const safeText = text.substring(0, 2000);
    if (!safeText.trim()) {
      return new Array(768).fill(0) as number[];
    }
    const vector = await this.embeddings.embedQuery(safeText);
    return vector;
  }

  async saveFile(
    filename: string,
    originalName: string,
    content: string,
  ): Promise<number> {
    console.log(
      `ğŸ”ª ì²­í‚¹ ì‹œì‘: ${originalName} (ì „ì²´ ê¸¸ì´: ${content.length}ì)`,
    );

    const splitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    });

    const chunks = await splitter.createDocuments([content]);
    console.log(`ğŸ§© ìƒì„±ëœ ì²­í¬ ê°œìˆ˜: ${chunks.length}ê°œ`);

    let savedCount = 0;
    for (const chunk of chunks) {
      const chunkContent = chunk.pageContent;
      const embedding = await this.getEmbedding(chunkContent);

      const newDocument = this.pdfRepository.create({
        filename,
        originalName,
        content: chunkContent,
        embedding,
      });

      await this.pdfRepository.save(newDocument);
      savedCount++;
    }
    return savedCount;
  }

  async search(question: string): Promise<SearchResult[]> {
    console.log(`ğŸ” ê²€ìƒ‰ ìš”ì²­: "${question}"`);
    const queryVector = await this.getEmbedding(question);

    const results: SearchResult[] = await this.pdfRepository.query(
      `
      SELECT 
        id, 
        filename, 
        "originalName", 
        content, 
        1 - (embedding <=> $1) as similarity
      FROM pdf_document
      ORDER BY embedding <=> $1 ASC
      LIMIT 4 
      `,
      [`[${queryVector.join(',')}]`],
    );
    return results;
  }

  // ğŸ‘‡ [ìˆ˜ì •] Redisê°€ ì ìš©ëœ ì±„íŒ… ë¡œì§
  async chat(question: string, sessionId?: string): Promise<string> {
    const safeSessionId = sessionId || 'default-session';
    // Redis Key ì •ì˜ (êµ¬ë¶„ì„ ìœ„í•´ prefix ì‚¬ìš©)
    const historyKey = `chat_history:${safeSessionId}`;
    const responseCacheKey = `response:${safeSessionId}:${question}`;

    console.log(`ğŸ’¬ ì§ˆë¬¸: "${question}" (Session: ${safeSessionId})`);

    // ---------------------------------------------------------
    // âš¡ï¸ 0. ì‘ë‹µ ìºì‹± (Response Caching)
    // ë˜‘ê°™ì€ ì§ˆë¬¸ì„ ë˜ í–ˆë‹¤ë©´, LLM ì—°ì‚° ì—†ì´ Redisì—ì„œ ë°”ë¡œ ë°˜í™˜
    // ---------------------------------------------------------
    const cachedResponse =
      await this.cacheManager.get<string>(responseCacheKey);
    if (cachedResponse) {
      console.log('âš¡ï¸ Redis ìºì‹œ íˆíŠ¸! (LLM ì—°ì‚° ìƒëµ)');
      return cachedResponse;
    }

    // ---------------------------------------------------------
    // 1. ê²€ìƒ‰ (Retrieval)
    // ---------------------------------------------------------
    const relevantDocs = await this.search(question);
    const context =
      relevantDocs.length > 0
        ? relevantDocs.map((doc) => doc.content).join('\n\n---\n\n')
        : 'ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';

    // ---------------------------------------------------------
    // 2. ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (Redis Memory Retrieval)
    // ---------------------------------------------------------
    // Redisì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
    const history =
      (await this.cacheManager.get<ChatMessage[]>(historyKey)) || [];

    // ìµœê·¼ 6ê°œ ë©”ì‹œì§€(3í„´)ë§Œ ìœ ì§€
    const recentHistory = history.slice(-6);

    // ëŒ€í™” ë‚´ì—­ í¬ë§·íŒ… (JSON -> String ë³€í™˜)
    const chatHistoryText = recentHistory
      .map((msg) => {
        const role = msg.role === 'user' ? 'ì‚¬ìš©ì' : 'AI ë¹„ì„œ';
        return `${role}: ${msg.content}`;
      })
      .join('\n');

    console.log(`ğŸ“œ ë¶ˆëŸ¬ì˜¨ ëŒ€í™” ë‚´ì—­:\n${chatHistoryText || '(ì—†ìŒ)'}`);

    // ---------------------------------------------------------
    // 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    // ---------------------------------------------------------
    const prompt = PromptTemplate.fromTemplate(`
      ### SYSTEM ROLE
      You are a professional AI assistant who speaks **ONLY Korean (í•œêµ­ì–´)**.
      Your task is to answer the user's question based on the provided [Context] AND [Chat History].

      ### ğŸš¨ CRITICAL RULES (MUST FOLLOW) ğŸš¨
      1. **SOURCE PRIORITY:** - First, check the **[Chat History]** to see if the user mentioned their name or previous topics.
         - Second, check the **[Context]** for document-related information.
      2. **LANGUAGE:** You must answer in **Korean** language only.
      3. **GROUNDING:** - If the answer is found in [Chat History], answer based on memory.
         - If the answer is found in [Context], answer based on the document.
         - If the answer is in **NEITHER**, say "ë¬¸ì„œë‚˜ ì´ì „ ëŒ€í™”ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

      ### DATA SOURCE
      [Chat History] (User's previous messages)
      {chat_history}

      [Context from Documents] (Search results)
      {context}

      ---------------------------------------------------

      ### USER INPUT
      [Question]
      {question}

      ### ğŸ“¢ FINAL INSTRUCTION
      Answer the question above in **Korean(í•œêµ­ì–´)**. 
      If the user asks "What is my name?", check the [Chat History] first.

      ë‹µë³€:
    `);

    // ---------------------------------------------------------
    // 4. ë‹µë³€ ìƒì„±
    // ---------------------------------------------------------
    const chain = prompt.pipe(this.chatModel).pipe(new StringOutputParser());
    const response = await chain.invoke({
      chat_history: chatHistoryText,
      context: context,
      question: question,
    });

    // ---------------------------------------------------------
    // 5. ëŒ€í™” ê¸°ë¡ ì €ì¥ (Redis Save)
    // ---------------------------------------------------------
    // ìƒˆ ëŒ€í™” ì¶”ê°€
    recentHistory.push({ role: 'user', content: question });
    recentHistory.push({ role: 'assistant', content: response });

    // Redisì— ëŒ€í™” ë‚´ì—­ ì €ì¥ (TTL: 24ì‹œê°„ ìœ ì§€)
    // awaitì€ í•„ìˆ˜ì…ë‹ˆë‹¤!
    await this.cacheManager.set(historyKey, recentHistory, 86400 * 1000);

    // í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë„ ìºì‹± (TTL: 1ì‹œê°„ ìœ ì§€)
    // ë‹¤ìŒì— ë˜‘ê°™ì€ ì§ˆë¬¸ í•˜ë©´ ë°”ë¡œ ëŒ€ë‹µí•¨
    await this.cacheManager.set(responseCacheKey, response, 3600 * 1000);

    console.log('âœ… ë‹µë³€ ì™„ë£Œ ë° Redis ì €ì¥ ì„±ê³µ');
    return response;
  }
}
